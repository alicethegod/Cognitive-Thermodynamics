{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yq12Y6MKprF0"
      },
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "This script implements the final, focused version of the Cognitive Thermodynamics\n",
        "analysis, centering the narrative on the core, robust findings: the macroscopic\n",
        "phase transition observed in ICE and the evolution of the underlying semantic\n",
        "potential energy landscape.\n",
        "\n",
        "Core Design Philosophy (Final Phase Transition Focus Version):\n",
        "1.  Focus on Robust Observables: Removes the unstable estimation of the internal\n",
        "    temperature T and the associated Smeta efficiency (Î±) analysis.\n",
        "2.  Highlight Phase Transition: Re-introduces the generalized logistic (sigmoid)\n",
        "    fit for the System Temperature (ICE) curve, providing the primary quantitative\n",
        "    evidence for a phase transition.\n",
        "3.  Objective Potential Energy: Retains the ultimate, theoretically-grounded\n",
        "    definition of energy (E_pot) as the unweighted norm of the semantic state vector.\n",
        "4.  Memory-Enhanced Analyzer: Uses the most stable analyzer design to ensure smooth\n",
        "    and continuous metric calculation.\n",
        "\"\"\"\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import copy\n",
        "import warnings\n",
        "from scipy.stats import entropy\n",
        "from scipy.optimize import curve_fit\n",
        "from scipy.spatial.distance import pdist, squareform\n",
        "import networkx as nx\n",
        "import os\n",
        "import json\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "\n",
        "# --- 1. Experiment Configuration ---\n",
        "CONFIG = {\n",
        "    \"train_subset_size\": 500,\n",
        "    \"epochs\": 100,\n",
        "    \"batch_size\": 64,\n",
        "    \"learning_rate\": 0.01,\n",
        "    \"model_h1_size\": 256,\n",
        "    \"model_h2_size\": 128,\n",
        "    \"analysis_interval\": 5,\n",
        "    \"analysis_sample_size\": 100,\n",
        "}\n",
        "\n",
        "# --- 2. Neural Network Model ---\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, h1_size=256, h2_size=128):\n",
        "        super(MLP, self).__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(28 * 28, h1_size), nn.ReLU(),\n",
        "            nn.Linear(h1_size, h2_size), nn.ReLU(),\n",
        "            nn.Linear(h2_size, 10)\n",
        "        )\n",
        "    def forward(self, x): return self.layers(x.view(-1, 28 * 28))\n",
        "    def get_hidden_activations(self, x):\n",
        "        activations = {}\n",
        "        x = x.view(-1, 28 * 28)\n",
        "        current_layer_input = x\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            current_layer_input = layer(current_layer_input)\n",
        "            if isinstance(layer, nn.ReLU):\n",
        "                activations[f'hidden_{i//2}'] = current_layer_input\n",
        "        return activations\n",
        "\n",
        "# --- 3. Cognitive Thermodynamics Analyzer (with Memory) ---\n",
        "class CognitiveThermodynamicsAnalyzer:\n",
        "    def __init__(self, initial_model, device):\n",
        "        self.device = device\n",
        "        self.memoized_paths = {} # Persistent cache\n",
        "        self.model = initial_model\n",
        "        self.update_graph(initial_model) # Initial graph build\n",
        "\n",
        "    def update_graph(self, model):\n",
        "        \"\"\"Dynamically updates the internal graph based on the new model state.\"\"\"\n",
        "        self.model = model\n",
        "        self.analyzer_model = copy.deepcopy(model).to('cpu')\n",
        "        self.linear_layers = [l for l in self.analyzer_model.layers if isinstance(l, nn.Linear)]\n",
        "        \n",
        "        G = nx.DiGraph()\n",
        "        for i in range(self.linear_layers[0].in_features): G.add_node(f\"0-{i}\", layer=0)\n",
        "        for i, l in enumerate(self.linear_layers):\n",
        "            for j in range(l.out_features): G.add_node(f\"{i+1}-{j}\", layer=i+1)\n",
        "            weights = torch.abs(l.weight.data.t())\n",
        "            probs = torch.softmax(weights, dim=1)\n",
        "            for u in range(l.in_features):\n",
        "                for v in range(l.out_features):\n",
        "                    p = probs[u, v].item()\n",
        "                    if p > 1e-9: G.add_edge(f\"{i}-{u}\", f\"{i+1}-{v}\", cost=1.0 - np.log(p))\n",
        "        \n",
        "        self.graph = G\n",
        "        self.grounding_nodes = {f\"{len(self.linear_layers)}-{i}\" for i in range(10)}\n",
        "        self.hidden_nodes = [n for n, d in self.graph.nodes(data=True) if 0 < d['layer'] < len(self.linear_layers)]\n",
        "\n",
        "    def _find_all_paths_dfs(self, start, targets):\n",
        "        memo_key = (start, tuple(sorted(list(targets))))\n",
        "        if memo_key in self.memoized_paths: return self.memoized_paths[memo_key]\n",
        "        paths, stack = [], [(start, [start], 0)]\n",
        "        while stack:\n",
        "            curr, path, cost = stack.pop()\n",
        "            if curr in targets: paths.append({'path': path, 'cost': cost}); continue\n",
        "            if len(path) > 8: continue\n",
        "            for neighbor in self.graph.neighbors(curr):\n",
        "                if neighbor not in path:\n",
        "                    stack.append((neighbor, path + [neighbor], cost + self.graph[curr][neighbor]['cost']))\n",
        "        self.memoized_paths[memo_key] = paths\n",
        "        return paths\n",
        "\n",
        "    def _calculate_metrics_for_node(self, node):\n",
        "        paths = self._find_all_paths_dfs(node, self.grounding_nodes)\n",
        "        if not paths: return float('inf'), float('inf')\n",
        "        costs = np.array([p['cost'] for p in paths])\n",
        "        conductances = 1.0 / costs\n",
        "        h_tse = 1.0 / np.sum(conductances) if np.sum(conductances) > 0 else float('inf')\n",
        "        importances = np.exp(-1.0 * costs)\n",
        "        groundingness = np.sum(importances)\n",
        "        probabilities = importances / groundingness if groundingness > 0 else []\n",
        "        h_sie = -np.sum(probabilities * np.log2(probabilities + 1e-9)) if probabilities.size > 0 else float('inf')\n",
        "        return h_tse, h_sie\n",
        "\n",
        "    def get_all_metrics_and_distributions(self):\n",
        "        htse_vals, hsie_vals, potential_energy_vals = [], [], []\n",
        "        if not self.hidden_nodes: return {}\n",
        "        \n",
        "        sample_size = min(CONFIG[\"analysis_sample_size\"], len(self.hidden_nodes))\n",
        "        sampled_nodes = np.random.choice(self.hidden_nodes, size=sample_size, replace=False)\n",
        "        \n",
        "        for node in sampled_nodes:\n",
        "            h_tse, h_sie = self._calculate_metrics_for_node(node)\n",
        "            if np.isfinite(h_tse) and np.isfinite(h_sie):\n",
        "                htse_vals.append(h_tse)\n",
        "                hsie_vals.append(h_sie)\n",
        "                potential_energy = np.sqrt(h_tse**2 + h_sie**2)\n",
        "                potential_energy_vals.append(potential_energy)\n",
        "        \n",
        "        metrics = {\n",
        "            \"avg_htse\": np.mean(htse_vals) if htse_vals else 0,\n",
        "            \"avg_hsie\": np.mean(hsie_vals) if hsie_vals else 0,\n",
        "            \"potential_energy_dist\": potential_energy_vals\n",
        "        }\n",
        "        return metrics\n",
        "\n",
        "    def analyze_macroscopic(self, test_loader):\n",
        "        self.model.eval()\n",
        "        prototypes = []\n",
        "        with torch.no_grad():\n",
        "            for i in range(10):\n",
        "                class_images = [img for img, label in test_loader.dataset if label == i][:100]\n",
        "                if not class_images: continue\n",
        "                class_tensor = torch.stack(class_images).to(self.device)\n",
        "                hidden_acts = self.model.get_hidden_activations(class_tensor)\n",
        "                last_hidden_key = f'hidden_{(len(self.linear_layers)-2)//2}'\n",
        "                if last_hidden_key in hidden_acts:\n",
        "                    class_rep = hidden_acts[last_hidden_key].mean(dim=0)\n",
        "                    prototypes.append(class_rep.cpu().numpy())\n",
        "        if len(prototypes) < 2: return 0\n",
        "        sim_matrix = 1 - squareform(pdist(np.array(prototypes), 'cosine'))\n",
        "        np.fill_diagonal(sim_matrix, 0)\n",
        "        prob_dist = sim_matrix / sim_matrix.sum() if sim_matrix.sum() > 0 else sim_matrix\n",
        "        return entropy(prob_dist.flatten())\n",
        "\n",
        "# --- 4. Training and Evaluation Functions ---\n",
        "def evaluate(model, dataloader, device):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in dataloader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            pred = model(data).argmax(dim=1, keepdim=True)\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "    return 100. * correct / len(dataloader.dataset)\n",
        "\n",
        "# --- 5. Main Experiment Workflow ---\n",
        "if __name__ == '__main__':\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
        "    mnist_train_full = datasets.MNIST('.', train=True, download=True, transform=transform)\n",
        "    mnist_test = datasets.MNIST('.', train=False, download=True, transform=transform)\n",
        "\n",
        "    train_subset = Subset(mnist_train_full, range(CONFIG['train_subset_size']))\n",
        "    train_loader = DataLoader(train_subset, batch_size=CONFIG['batch_size'], shuffle=True)\n",
        "    test_loader = DataLoader(mnist_test, batch_size=CONFIG['batch_size'])\n",
        "\n",
        "    print(f\"Experiment Setup: Training on {len(train_subset)} samples for {CONFIG['epochs']} epochs.\")\n",
        "    \n",
        "    model = MLP(CONFIG['model_h1_size'], CONFIG['model_h2_size']).to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=CONFIG['learning_rate'])\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    analyzer = CognitiveThermodynamicsAnalyzer(model, device)\n",
        "    \n",
        "    history = {\n",
        "        \"epoch\": [], \"loss\": [], \"train_acc\": [], \"test_acc\": [], \n",
        "        \"htse\": [], \"hsie\": [], \"ice\": [],\n",
        "        \"initial_potential_energy_dist\": [], \"final_potential_energy_dist\": [],\n",
        "    }\n",
        "    \n",
        "    print(\"Analyzing initial (t=0) random state...\")\n",
        "    initial_metrics = analyzer.get_all_metrics_and_distributions()\n",
        "    history[\"initial_potential_energy_dist\"] = initial_metrics.get(\"potential_energy_dist\", [])\n",
        "    \n",
        "    pbar = tqdm(range(1, CONFIG['epochs'] + 1), desc=\"Training Progress\")\n",
        "    for epoch in pbar:\n",
        "        model.train()\n",
        "        epoch_losses = []\n",
        "        for data, target in train_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            output = model(data)\n",
        "            loss = criterion(output, target)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            epoch_losses.append(loss.item())\n",
        "        avg_epoch_loss = np.mean(epoch_losses)\n",
        "\n",
        "        if epoch % CONFIG['analysis_interval'] == 0 or epoch == CONFIG['epochs']:\n",
        "            analyzer.update_graph(model)\n",
        "            \n",
        "            train_acc = evaluate(model, train_loader, device)\n",
        "            test_acc = evaluate(model, test_loader, device)\n",
        "            \n",
        "            current_metrics = analyzer.get_all_metrics_and_distributions()\n",
        "            ice_current = analyzer.analyze_macroscopic(test_loader)\n",
        "\n",
        "            history[\"epoch\"].append(epoch)\n",
        "            history[\"loss\"].append(avg_epoch_loss)\n",
        "            history[\"train_acc\"].append(train_acc)\n",
        "            history[\"test_acc\"].append(test_acc)\n",
        "            history[\"htse\"].append(current_metrics.get(\"avg_htse\", 0))\n",
        "            history[\"hsie\"].append(current_metrics.get(\"avg_hsie\", 0))\n",
        "            history[\"ice\"].append(ice_current)\n",
        "            \n",
        "            pbar.set_postfix({\"Loss\": f\"{avg_epoch_loss:.2f}\", \"ICE\": f\"{ice_current:.2f}\"})\n",
        "\n",
        "    print(\"Analyzing final (t=100) ordered state...\")\n",
        "    final_metrics = analyzer.get_all_metrics_and_distributions()\n",
        "    history[\"final_potential_energy_dist\"] = final_metrics.get(\"potential_energy_dist\", [])\n",
        "    \n",
        "    results_dir = \"results_phase_transition\"\n",
        "    os.makedirs(results_dir, exist_ok=True)\n",
        "    results_path = os.path.join(results_dir, \"results.json\")\n",
        "    with open(results_path, 'w') as f: json.dump(history, f, indent=4)\n",
        "    print(f\"\\nExperiment data successfully saved to: {results_path}\")\n",
        "\n",
        "    # --- 7. Results Visualization ---\n",
        "    plt.style.use('seaborn-v0_8-whitegrid')\n",
        "    # Changed to 2x2 grid and adjusted figsize\n",
        "    fig, axs = plt.subplots(2, 2, figsize=(20, 12)) \n",
        "    fig.suptitle('Cognitive Thermodynamics: Analysis of Phase Transition', fontsize=22, y=0.98)\n",
        "    \n",
        "    epochs = np.array(history['epoch'])\n",
        "    \n",
        "    # Plot 1 (Position [0, 0]): Accuracy & Loss Dynamics\n",
        "    ax1 = axs[0, 0]\n",
        "    ax1_twin = ax1.twinx()\n",
        "    ax1.plot(epochs, history['train_acc'], 'b-o', label='Training Accuracy')\n",
        "    ax1.plot(epochs, history['test_acc'], 'r-s', label='Test Accuracy')\n",
        "    ax1_twin.plot(epochs, history['loss'], 'g--p', label='Training Loss')\n",
        "    ax1.set_title('1. Accuracy & Loss Dynamics', fontsize=16)\n",
        "    ax1.set_ylabel('Accuracy (%)', fontsize=12)\n",
        "    ax1_twin.set_ylabel('Loss', fontsize=12, color='g')\n",
        "    ax1.legend(loc='center left'); ax1_twin.legend(loc='center right')\n",
        "\n",
        "    # Plot 2 (Position [0, 1]): Semantic Temperature (ICE) Dynamics with Logistic Fit\n",
        "    ax2 = axs[0, 1]\n",
        "    ice_data = np.array(history['ice'])\n",
        "    ax2.plot(epochs, ice_data, 'm-^', label='Semantic Temperature (ICE)')\n",
        "    \n",
        "    def generalized_logistic_function(t, L, A, k, t0):\n",
        "        return L + A / (1 + np.exp(-k * (t - t0)))\n",
        "\n",
        "    min_ice_idx = np.argmin(ice_data)\n",
        "    if min_ice_idx < len(ice_data) - 3:\n",
        "        epochs_to_fit = epochs[min_ice_idx:]\n",
        "        ice_to_fit = ice_data[min_ice_idx:]\n",
        "        props = dict(boxstyle='round', facecolor='wheat', alpha=0.6)\n",
        "        try:\n",
        "            L_guess = np.min(ice_to_fit)\n",
        "            A_guess = np.max(ice_to_fit) - L_guess\n",
        "            half_way_val = L_guess + A_guess / 2\n",
        "            try:\n",
        "                t0_index = np.where(ice_to_fit >= half_way_val)[0][0]\n",
        "                t0_guess = epochs_to_fit[t0_index]\n",
        "            except IndexError:\n",
        "                t0_guess = np.median(epochs_to_fit)\n",
        "            k_guess = 1.0\n",
        "            initial_guesses = [L_guess, A_guess, k_guess, t0_guess]\n",
        "            bounds = ([np.min(ice_to_fit)*0.9, A_guess*0.5, 1e-5, epochs_to_fit[0]], \n",
        "                      [np.min(ice_to_fit)*1.1, A_guess*1.5, 5.0, epochs_to_fit[-1]])\n",
        "            \n",
        "            popt, _ = curve_fit(generalized_logistic_function, epochs_to_fit, ice_to_fit, p0=initial_guesses, bounds=bounds, maxfev=10000)\n",
        "            r_squared = 1 - (np.sum((ice_to_fit - generalized_logistic_function(epochs_to_fit, *popt))**2) / np.sum((ice_to_fit - np.mean(ice_to_fit))**2))\n",
        "            \n",
        "            epochs_fine = np.linspace(epochs_to_fit[0], epochs_to_fit[-1], 200)\n",
        "            ax2.plot(epochs_fine, generalized_logistic_function(epochs_fine, *popt), 'g--', lw=2.5, label=f'Logistic Fit (RÂ²={r_squared:.3f})')\n",
        "            result_text = (f\"Phase Transition Analysis:\\n\"\n",
        "                           f\"RÂ² = {r_squared:.4f}\\nMax Temp: {popt[0]+popt[1]:.3f}\\n\"\n",
        "                           f\"Speed (k): {popt[2]:.3f}\\nCritical Point (t0): {popt[3]:.2f} Epochs\")\n",
        "            ax2.text(0.05, 0.95, result_text, transform=ax2.transAxes, fontsize=10, va='top', bbox=props)\n",
        "        except (RuntimeError, ValueError) as e:\n",
        "            ax2.text(0.05, 0.95, f\"Logistic fit failed\", transform=ax2.transAxes, fontsize=10, va='top', bbox=props)\n",
        "    \n",
        "    ax2.set_title('2. Semantic Temperature (ICE) Dynamics', fontsize=16)\n",
        "    ax2.set_ylabel('Semantic Temperature (Entropy)', fontsize=12)\n",
        "    ax2.legend()\n",
        "\n",
        "    # Plot 3 (Position [1, 0]): Microscopic Entropy Dynamics\n",
        "    ax3 = axs[1, 0]\n",
        "    ax3.plot(epochs, history['htse'], 'c-p', label=\"H'_TSE (Cognitive Cost)\")\n",
        "    ax3.plot(epochs, history['hsie'], 'y-h', label=\"H'_SIE (Structural Robustness)\")\n",
        "    ax3.set_title('3. Microscopic Entropy Dynamics', fontsize=16)\n",
        "    ax3.set_ylabel('Entropy Value', fontsize=12)\n",
        "    ax3.legend(loc='center right')\n",
        "    \n",
        "    # Plot 4 (Position [1, 1]): Semantic Potential Energy Distribution\n",
        "    ax4 = axs[1, 1]\n",
        "    energy_initial = history['initial_potential_energy_dist']\n",
        "    energy_final = history['final_potential_energy_dist']\n",
        "    \n",
        "    if energy_initial and energy_final:\n",
        "        combined_energy = np.hstack((energy_initial, energy_final))\n",
        "        combined_energy = combined_energy[np.isfinite(combined_energy)]\n",
        "        if combined_energy.size > 0:\n",
        "            bins = np.histogram(combined_energy, bins=25)[1]\n",
        "            ax4.hist(energy_initial, bins=bins, alpha=0.7, label='Initial State (High E_pot)', color='blue', density=True)\n",
        "            ax4.hist(energy_final, bins=bins, alpha=0.7, label='Final State (Low E_pot)', color='red', density=True)\n",
        "            ax4.set_yscale('log')\n",
        "            ax4.legend()\n",
        "        else:\n",
        "            ax4.text(0.5, 0.5, \"No finite energy data to plot.\", ha='center', va='center')\n",
        "    else:\n",
        "        if not energy_initial: print(\"Warning: Initial potential energy distribution is empty.\")\n",
        "        if not energy_final: print(\"Warning: Final potential energy distribution is empty.\")\n",
        "        ax4.text(0.5, 0.5, \"Energy data not available.\", ha='center', va='center')\n",
        "        \n",
        "    ax4.set_title('4. Objective Potential Energy Distribution', fontsize=16)\n",
        "    ax4.set_ylabel('Probability Density', fontsize=12)\n",
        "\n",
        "    # General labeling and grid for the 2x2 layout\n",
        "    for i, j in [(0,0), (0,1), (1,0), (1,1)]:\n",
        "        axs[i,j].set_xlabel('Epochs', fontsize=12)\n",
        "        axs[i,j].grid(True, which=\"both\", ls=\"--\")\n",
        "    \n",
        "    axs[1,1].set_xlabel('E_pot (Unweighted Norm of State Vector)', fontsize=12) # Specific label for energy plot\n",
        "\n",
        "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "    plt.show()\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
