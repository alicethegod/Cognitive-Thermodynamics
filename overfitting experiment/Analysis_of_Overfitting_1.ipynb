{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "This script uses a PyTorch neural network to fit a noisy cosine curve,\n",
    "aiming to create and analyze the phenomena of \"underfitting\", \"good fitting\", and \"overfitting\".\n",
    "\n",
    "Core Highlights:\n",
    "1. Use neural network models of varying complexity to correspond to the three fitting states.\n",
    "2. For each trained model, use the previously designed Cognitive Theory Analyzer (TheoryAnalyzer)\n",
    "   to calculate the H'_TSE and H'_SIE metrics.\n",
    "3. Visualize the model's fitting performance alongside the theoretical analysis metrics to intuitively\n",
    "   demonstrate the relationship between the model's external behavior and its internal structural measures.\n",
    "\n",
    "Expected Observations:\n",
    "- Underfitting Model: Simple structure, low cognitive cost (H'_TSE), but poor fitting performance.\n",
    "- Good Fit Model: Moderate structural complexity, relatively low cognitive cost, and good fitting performance.\n",
    "- Overfitting Model: Most complex structure, creating redundant paths to \"memorize\" data, leading to a\n",
    "  significantly higher cognitive cost (H'_TSE).\n",
    "\"\"\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# --- 1. Data Preparation ---\n",
    "def true_fun(X):\n",
    "    return np.cos(1.5 * np.pi * X)\n",
    "\n",
    "np.random.seed(0)\n",
    "n_samples = 30\n",
    "X_np = np.sort(np.random.rand(n_samples))\n",
    "y_np = true_fun(X_np) + np.random.randn(n_samples) * 0.1\n",
    "\n",
    "# Convert to PyTorch Tensors\n",
    "X_tensor = torch.FloatTensor(X_np).unsqueeze(1)\n",
    "y_tensor = torch.FloatTensor(y_np).unsqueeze(1)\n",
    "\n",
    "\n",
    "# --- 2. Neural Network Model Definition ---\n",
    "class RegressionMLP(nn.Module):\n",
    "    \"\"\"A Multi-Layer Perceptron for regression tasks\"\"\"\n",
    "    def __init__(self, layer_sizes):\n",
    "        super(RegressionMLP, self).__init__()\n",
    "        layers = []\n",
    "        for i in range(len(layer_sizes) - 2):\n",
    "            layers.append(nn.Linear(layer_sizes[i], layer_sizes[i+1]))\n",
    "            layers.append(nn.ReLU())\n",
    "        layers.append(nn.Linear(layer_sizes[-2], layer_sizes[-1]))\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "\n",
    "# --- 3. Cognitive Theory Analyzer (Adapted for Regression Models) ---\n",
    "class TheoryAnalyzer:\n",
    "    \"\"\"\n",
    "    Cognitive Theory Analyzer, adapted for regression models.\n",
    "    - Grounding Node: The single output node of the model.\n",
    "    - Graph Construction: Can handle RegressionMLP models with any number of layers.\n",
    "    \"\"\"\n",
    "    def __init__(self, model):\n",
    "        model_copy = copy.deepcopy(model)\n",
    "        self.model = model_copy.to('cpu')\n",
    "        self.linear_layers = [l for l in self.model.layers if isinstance(l, nn.Linear)]\n",
    "        self.graph = self._build_graph()\n",
    "        # For regression tasks, the grounding node is the unique output node\n",
    "        self.grounding_nodes = {f\"{len(self.linear_layers)}-0\"}\n",
    "        self.hidden_nodes = self._get_hidden_nodes()\n",
    "        self.memoized_paths = {}\n",
    "\n",
    "    def _build_graph(self):\n",
    "        G = nx.DiGraph()\n",
    "        # Input layer nodes\n",
    "        for i in range(self.linear_layers[0].in_features):\n",
    "            G.add_node(f\"0-{i}\", layer=0)\n",
    "        # Hidden and output layer nodes\n",
    "        for i, l in enumerate(self.linear_layers):\n",
    "            for j in range(l.out_features):\n",
    "                G.add_node(f\"{i+1}-{j}\", layer=i+1)\n",
    "            weights = torch.abs(l.weight.data.t())\n",
    "            probs = torch.softmax(weights, dim=1)\n",
    "            for u in range(l.in_features):\n",
    "                for v in range(l.out_features):\n",
    "                    p = probs[u, v].item()\n",
    "                    if p > 1e-9:\n",
    "                        G.add_edge(f\"{i}-{u}\", f\"{i+1}-{v}\", cost=1.0 - np.log(p))\n",
    "        return G\n",
    "\n",
    "    def _get_hidden_nodes(self):\n",
    "        # Hidden nodes are in all layers except the input (0) and output (len) layers\n",
    "        num_layers = len(self.linear_layers)\n",
    "        return [n for n, d in self.graph.nodes(data=True) if 0 < d['layer'] < num_layers]\n",
    "\n",
    "    def find_all_paths_dfs(self, start, targets):\n",
    "        memo_key = (start, tuple(sorted(list(targets))))\n",
    "        if memo_key in self.memoized_paths: return self.memoized_paths[memo_key]\n",
    "\n",
    "        paths, stack = [], [(start, [start], 0)]\n",
    "        while stack:\n",
    "            curr, path, cost = stack.pop()\n",
    "            if curr in targets:\n",
    "                paths.append({'path': path, 'cost': cost})\n",
    "                continue\n",
    "            if len(path) > (len(self.linear_layers) + 2): continue # Limit path depth\n",
    "            for neighbor in self.graph.neighbors(curr):\n",
    "                if neighbor not in path:\n",
    "                    stack.append((neighbor, path + [neighbor], cost + self.graph[curr][neighbor]['cost']))\n",
    "        self.memoized_paths[memo_key] = paths\n",
    "        return paths\n",
    "\n",
    "    def calculate_metrics_for_node(self, node):\n",
    "        paths = self.find_all_paths_dfs(node, self.grounding_nodes)\n",
    "        if not paths: return float('inf'), float('inf')\n",
    "\n",
    "        costs = np.array([p['cost'] for p in paths])\n",
    "        importances = np.exp(-1.0 * costs)\n",
    "        groundingness = np.sum(importances)\n",
    "\n",
    "        conductances = 1.0 / costs\n",
    "        h_tse = 1.0 / np.sum(conductances) if np.sum(conductances) > 0 else float('inf')\n",
    "\n",
    "        probabilities = importances / groundingness if groundingness > 0 else []\n",
    "        if probabilities.size > 0:\n",
    "             h_sie = -np.sum(probabilities * np.log2(probabilities + 1e-9))\n",
    "        else:\n",
    "            h_sie = float('inf')\n",
    "\n",
    "        return h_tse, h_sie\n",
    "\n",
    "    def analyze(self):\n",
    "        htse_vals, hsie_vals = [], []\n",
    "        if not self.hidden_nodes: return 0, 0 # If no hidden layers, metrics are 0\n",
    "\n",
    "        # Analyze all hidden nodes\n",
    "        for node in tqdm(self.hidden_nodes, desc=f\"Analyzing Cognitive Metrics\", leave=False):\n",
    "            h_tse, h_sie = self.calculate_metrics_for_node(node)\n",
    "            if np.isfinite(h_tse) and np.isfinite(h_sie):\n",
    "                htse_vals.append(h_tse)\n",
    "                hsie_vals.append(h_sie)\n",
    "\n",
    "        avg_htse = np.mean(htse_vals) if htse_vals else 0\n",
    "        avg_hsie = np.mean(hsie_vals) if hsie_vals else 0\n",
    "        return avg_htse, avg_hsie\n",
    "\n",
    "\n",
    "# --- 4. Main Training and Analysis Workflow ---\n",
    "\n",
    "# Define three experimental configurations\n",
    "model_configs = {\n",
    "    \"Underfitting\": {\n",
    "        \"layers\": [1, 5, 3, 1],  # Still a simple structure, but with more path diversity\n",
    "        \"epochs\": 1000,\n",
    "        \"lr\": 0.01\n",
    "    },\n",
    "    \"Good Fit\": {\n",
    "        \"layers\": [1, 64, 64, 1], # A moderately complex model\n",
    "        \"epochs\": 5000,\n",
    "        \"lr\": 0.01\n",
    "    },\n",
    "    \"Overfitting\": {\n",
    "        \"layers\": [1, 128, 128, 128, 1], # A very complex model\n",
    "        \"epochs\": 40000, # Train for a very long time\n",
    "        \"lr\": 0.001\n",
    "    }\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "# Training and analysis loop\n",
    "for name, config in model_configs.items():\n",
    "    print(f\"--- Training Model for: {name} ---\")\n",
    "    model = RegressionMLP(config[\"layers\"])\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config[\"lr\"])\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    for epoch in tqdm(range(config[\"epochs\"]), desc=f\"Training {name}\"):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_tensor)\n",
    "        loss = criterion(outputs, y_tensor)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Analyze after training\n",
    "    analyzer = TheoryAnalyzer(model)\n",
    "    avg_htse, avg_hsie = analyzer.analyze()\n",
    "\n",
    "    # Save results\n",
    "    results[name] = {\n",
    "        \"model\": model,\n",
    "        \"loss\": loss.item(),\n",
    "        \"htse\": avg_htse,\n",
    "        \"hsie\": avg_hsie\n",
    "    }\n",
    "    print(f\"Final Loss: {loss.item():.4f}, Avg H'_TSE: {avg_htse:.4f}, Avg H'_SIE: {avg_hsie:.4f}\\n\")\n",
    "\n",
    "\n",
    "# --- 5. Visualization ---\n",
    "plt.figure(figsize=(20, 6))\n",
    "plt.suptitle(\"NN Curve Fitting: Underfitting vs. Good Fit vs. Overfitting\", fontsize=18)\n",
    "\n",
    "for i, name in enumerate(model_configs.keys()):\n",
    "    ax = plt.subplot(1, 3, i + 1)\n",
    "    model = results[name][\"model\"]\n",
    "\n",
    "    # Prepare test data for plotting\n",
    "    X_test = torch.FloatTensor(np.linspace(0, 1, 100)).unsqueeze(1)\n",
    "    with torch.no_grad():\n",
    "        y_pred = model(X_test)\n",
    "\n",
    "    # Plotting\n",
    "    ax.scatter(X_np, y_np, edgecolor=\"b\", s=20, label=\"Training Samples\")\n",
    "    ax.plot(np.linspace(0, 1, 100), true_fun(np.linspace(0, 1, 100)), 'g-', label=\"True Function\")\n",
    "    ax.plot(X_test.numpy(), y_pred.numpy(), 'r-', label=\"Model Fit\")\n",
    "\n",
    "    ax.set_xlim((0, 1))\n",
    "    ax.set_ylim((-1.5, 1.5))\n",
    "    ax.set_xlabel(\"x\")\n",
    "    ax.set_ylabel(\"y\")\n",
    "    ax.legend()\n",
    "\n",
    "    # Display all metrics in the title\n",
    "    title_text = (\n",
    "        f\"{name}\\n\"\n",
    "        f\"Final MSE: {results[name]['loss']:.3f}\\n\"\n",
    "        f\"Avg H'_TSE: {results[name]['htse']:.3f} (Cognitive Cost)\\n\"\n",
    "        f\"Avg H'_SIE: {results[name]['hsie']:.3f} (Robustness)\"\n",
    "    )\n",
    "    ax.set_title(title_text, fontsize=12)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
